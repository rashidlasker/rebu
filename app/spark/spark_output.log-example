2019-05-08 19:14:54,350 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-05-08 19:14:54,968 INFO spark.SparkContext: Running Spark version 2.4.1
2019-05-08 19:14:54,992 INFO spark.SparkContext: Submitted application: PopularItems
2019-05-08 19:14:55,030 INFO spark.SecurityManager: Changing view acls to: root
2019-05-08 19:14:55,030 INFO spark.SecurityManager: Changing modify acls to: root
2019-05-08 19:14:55,030 INFO spark.SecurityManager: Changing view acls groups to: 
2019-05-08 19:14:55,030 INFO spark.SecurityManager: Changing modify acls groups to: 
2019-05-08 19:14:55,030 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2019-05-08 19:14:55,227 INFO util.Utils: Successfully started service 'sparkDriver' on port 40527.
2019-05-08 19:14:55,246 INFO spark.SparkEnv: Registering MapOutputTracker
2019-05-08 19:14:55,261 INFO spark.SparkEnv: Registering BlockManagerMaster
2019-05-08 19:14:55,264 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-05-08 19:14:55,264 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2019-05-08 19:14:55,273 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-14a8c51a-1913-403a-9cd3-c43c9fe95095
2019-05-08 19:14:55,289 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
2019-05-08 19:14:55,302 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2019-05-08 19:14:55,363 INFO util.log: Logging initialized @2080ms
2019-05-08 19:14:55,415 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-05-08 19:14:55,431 INFO server.Server: Started @2149ms
2019-05-08 19:14:55,445 INFO server.AbstractConnector: Started ServerConnector@7baad265{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-05-08 19:14:55,445 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2019-05-08 19:14:55,469 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35b3ea53{/jobs,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,470 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bf94a26{/jobs/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,471 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d2c4b9a{/jobs/job,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,474 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72a5160d{/jobs/job/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,475 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6dcaf358{/stages,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,476 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a99d9f5{/stages/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,477 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ff1f7fc{/stages/stage,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,479 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ba6c4d3{/stages/stage/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,480 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bb88e4c{/stages/pool,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,481 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4c1d45{/stages/pool/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,482 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2225e73d{/storage,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,483 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f082ff6{/storage/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,484 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@cd0ab5{/storage/rdd,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,485 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab199dc{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,486 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@572091b8{/environment,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,487 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19197335{/environment/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,488 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e94a01{/executors,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,489 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27c04299{/executors/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,490 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@743eba88{/executors/threadDump,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,491 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@266cefe1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,497 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1dee006f{/static,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,498 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698c3229{/,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,499 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@223f1b70{/api,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,500 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@695e1c6b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,501 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20e7a2b4{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,503 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-master:4040
2019-05-08 19:14:55,573 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2019-05-08 19:14:55,621 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.17.0.4:7077 after 32 ms (0 ms spent in bootstraps)
2019-05-08 19:14:55,713 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190508191455-0050
2019-05-08 19:14:55,719 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20190508191455-0050/0 on worker-20190508172652-172.17.0.6-8881 (172.17.0.6:8881) with 2 core(s)
2019-05-08 19:14:55,722 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20190508191455-0050/0 on hostPort 172.17.0.6:8881 with 2 core(s), 512.0 MB RAM
2019-05-08 19:14:55,729 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20190508191455-0050/0 is now RUNNING
2019-05-08 19:14:55,741 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34053.
2019-05-08 19:14:55,742 INFO netty.NettyBlockTransferService: Server created on spark-master:34053
2019-05-08 19:14:55,744 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-05-08 19:14:55,768 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 34053, None)
2019-05-08 19:14:55,771 INFO storage.BlockManagerMasterEndpoint: Registering block manager spark-master:34053 with 366.3 MB RAM, BlockManagerId(driver, spark-master, 34053, None)
2019-05-08 19:14:55,773 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 34053, None)
2019-05-08 19:14:55,774 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 34053, None)
2019-05-08 19:14:55,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23d30e60{/metrics/json,null,AVAILABLE,@Spark}
2019-05-08 19:14:55,924 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-05-08 19:14:56,642 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 420.1 KB, free 365.9 MB)
2019-05-08 19:14:56,705 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KB, free 365.9 MB)
2019-05-08 19:14:56,707 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:34053 (size: 37.1 KB, free: 366.3 MB)
2019-05-08 19:14:56,710 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
2019-05-08 19:14:56,844 INFO mapred.FileInputFormat: Total input files to process : 1
2019-05-08 19:14:57,028 INFO spark.SparkContext: Starting job: collect at /tmp/data/spark.py:28
2019-05-08 19:14:57,047 INFO scheduler.DAGScheduler: Registering RDD 3 (distinct at /tmp/data/spark.py:10)
2019-05-08 19:14:57,048 INFO scheduler.DAGScheduler: Registering RDD 7 (groupByKey at /tmp/data/spark.py:13)
2019-05-08 19:14:57,049 INFO scheduler.DAGScheduler: Registering RDD 11 (groupByKey at /tmp/data/spark.py:20)
2019-05-08 19:14:57,051 INFO scheduler.DAGScheduler: Got job 0 (collect at /tmp/data/spark.py:28) with 2 output partitions
2019-05-08 19:14:57,051 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at /tmp/data/spark.py:28)
2019-05-08 19:14:57,051 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
2019-05-08 19:14:57,053 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
2019-05-08 19:14:57,057 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /tmp/data/spark.py:10), which has no missing parents
2019-05-08 19:14:57,089 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.9 KB, free 365.8 MB)
2019-05-08 19:14:57,093 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 365.8 MB)
2019-05-08 19:14:57,095 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:34053 (size: 7.0 KB, free: 366.3 MB)
2019-05-08 19:14:57,096 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:14:57,110 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /tmp/data/spark.py:10) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:14:57,114 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
2019-05-08 19:14:57,826 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.6:49978) with ID 0
2019-05-08 19:14:57,843 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.17.0.6, executor 0, partition 0, PROCESS_LOCAL, 7886 bytes)
2019-05-08 19:14:57,846 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.17.0.6, executor 0, partition 1, PROCESS_LOCAL, 7886 bytes)
2019-05-08 19:14:57,942 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.6:37003 with 93.3 MB RAM, BlockManagerId(0, 172.17.0.6, 37003, None)
2019-05-08 19:14:58,226 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.6:37003 (size: 7.0 KB, free: 93.3 MB)
2019-05-08 19:14:58,358 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.6:37003 (size: 37.1 KB, free: 93.3 MB)
2019-05-08 19:14:59,566 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1719 ms on 172.17.0.6 (executor 0) (1/2)
2019-05-08 19:14:59,568 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1737 ms on 172.17.0.6 (executor 0) (2/2)
2019-05-08 19:14:59,569 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-05-08 19:14:59,574 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42771
2019-05-08 19:14:59,580 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (distinct at /tmp/data/spark.py:10) finished in 2.509 s
2019-05-08 19:14:59,580 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:14:59,581 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:14:59,582 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 2, ResultStage 3)
2019-05-08 19:14:59,582 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:14:59,585 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /tmp/data/spark.py:13), which has no missing parents
2019-05-08 19:14:59,593 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.7 KB, free 365.8 MB)
2019-05-08 19:14:59,597 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 365.8 MB)
2019-05-08 19:14:59,598 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:34053 (size: 6.9 KB, free: 366.3 MB)
2019-05-08 19:14:59,599 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:14:59,600 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /tmp/data/spark.py:13) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:14:59,600 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
2019-05-08 19:14:59,605 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.17.0.6, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2019-05-08 19:14:59,606 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.17.0.6, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2019-05-08 19:14:59,650 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.6:37003 (size: 6.9 KB, free: 93.3 MB)
2019-05-08 19:14:59,673 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.17.0.6:49978
2019-05-08 19:14:59,719 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:34053 in memory (size: 7.0 KB, free: 366.3 MB)
2019-05-08 19:14:59,726 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.17.0.6:37003 in memory (size: 7.0 KB, free: 93.3 MB)
2019-05-08 19:14:59,804 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 200 ms on 172.17.0.6 (executor 0) (1/2)
2019-05-08 19:14:59,806 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 200 ms on 172.17.0.6 (executor 0) (2/2)
2019-05-08 19:14:59,806 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-05-08 19:14:59,808 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (groupByKey at /tmp/data/spark.py:13) finished in 0.218 s
2019-05-08 19:14:59,808 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:14:59,808 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:14:59,808 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
2019-05-08 19:14:59,808 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:14:59,809 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/spark.py:20), which has no missing parents
2019-05-08 19:14:59,813 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KB, free 365.8 MB)
2019-05-08 19:14:59,815 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 365.8 MB)
2019-05-08 19:14:59,816 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:34053 (size: 8.0 KB, free: 366.2 MB)
2019-05-08 19:14:59,817 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:14:59,818 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/spark.py:20) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:14:59,818 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
2019-05-08 19:14:59,819 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 172.17.0.6, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2019-05-08 19:14:59,820 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 172.17.0.6, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2019-05-08 19:14:59,835 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.6:37003 (size: 8.0 KB, free: 93.2 MB)
2019-05-08 19:14:59,842 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.17.0.6:49978
2019-05-08 19:14:59,905 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 86 ms on 172.17.0.6 (executor 0) (1/2)
2019-05-08 19:14:59,910 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 91 ms on 172.17.0.6 (executor 0) (2/2)
2019-05-08 19:14:59,910 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-05-08 19:14:59,911 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (groupByKey at /tmp/data/spark.py:20) finished in 0.100 s
2019-05-08 19:14:59,912 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:14:59,912 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:14:59,912 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
2019-05-08 19:14:59,912 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:14:59,913 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[14] at collect at /tmp/data/spark.py:28), which has no missing parents
2019-05-08 19:14:59,917 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.2 KB, free 365.8 MB)
2019-05-08 19:14:59,919 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KB, free 365.8 MB)
2019-05-08 19:14:59,920 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:34053 (size: 5.9 KB, free: 366.2 MB)
2019-05-08 19:14:59,920 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:14:59,922 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[14] at collect at /tmp/data/spark.py:28) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:14:59,922 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
2019-05-08 19:14:59,925 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 172.17.0.6, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
2019-05-08 19:14:59,925 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 172.17.0.6, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
2019-05-08 19:14:59,940 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.6:37003 (size: 5.9 KB, free: 93.2 MB)
2019-05-08 19:14:59,947 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.6:49978
2019-05-08 19:15:00,002 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 78 ms on 172.17.0.6 (executor 0) (1/2)
2019-05-08 19:15:00,007 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 82 ms on 172.17.0.6 (executor 0) (2/2)
2019-05-08 19:15:00,008 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-05-08 19:15:00,009 INFO scheduler.DAGScheduler: ResultStage 3 (collect at /tmp/data/spark.py:28) finished in 0.094 s
2019-05-08 19:15:00,013 INFO scheduler.DAGScheduler: Job 0 finished: collect at /tmp/data/spark.py:28, took 2.984630 s
coview ('1', '2') count 3
Popular items done
2019-05-08 19:15:00,026 INFO server.AbstractConnector: Stopped Spark@7baad265{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-05-08 19:15:00,028 INFO ui.SparkUI: Stopped Spark web UI at http://spark-master:4040
2019-05-08 19:15:00,030 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
2019-05-08 19:15:00,031 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2019-05-08 19:15:00,041 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2019-05-08 19:15:00,065 INFO memory.MemoryStore: MemoryStore cleared
2019-05-08 19:15:00,066 INFO storage.BlockManager: BlockManager stopped
2019-05-08 19:15:00,067 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
2019-05-08 19:15:00,071 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2019-05-08 19:15:00,098 INFO spark.SparkContext: Successfully stopped SparkContext
2019-05-08 19:15:01,038 INFO util.ShutdownHookManager: Shutdown hook called
2019-05-08 19:15:01,039 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a1894d62-0d11-4fe1-b00f-11d730aca3ca
2019-05-08 19:15:01,043 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-02e1c242-dcc4-4db7-b7e3-59536df79cf0
2019-05-08 19:15:01,047 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a1894d62-0d11-4fe1-b00f-11d730aca3ca/pyspark-475c6695-9e38-4015-a068-5501c798d2b7
